{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "mt2021_hw4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcmmZwMfL8Te"
      },
      "source": [
        "# Homework 4\n",
        "\n",
        "**How to submit.** For this homework, submit this `.ipynb` file with your answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiwof0fQL8Tn"
      },
      "source": [
        "### Task 1: Importance of data (2 points)\n",
        "\n",
        "As we have discussed before, language is very diverse. Machine translation models work quite well in limited settings, but a single model is rarely good at dealing with very diverse kinds of texts. Therefore, it is important to keep in mind what data the model is trained on and what data it is applied to. In this task, you will verify that a model trained on a particular type of texts will perform worse when it has to translate texts of a different domain.\n",
        "\n",
        "**Subtask 1 (1 point).** You have two ET$\\rightarrow$EN Fairseq models (`/gpfs/hpc/projects/nlpgroup/mt2021/data/model-comparison`): one was trained on a corpus of legislative documents (DGT), and the other on a corpus of movie and TV subtitles (OpenSubtitles). You also have two test sets, one from the DGT corpus and one from OpenSubtitles. Preprocess the sets (the SentencePiece model is also provided), translate each of the test sets with each of the models, postprocess. Measure BLEU score for each of the four translations (using sacreBLEU, in the same way you did in Homework 3). Report the results, explain what you see.\n",
        "\n",
        "**Hint.** Translation may be faster if you increase batch size, e.g. `--batch-size 32`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYqw2g7xL8To"
      },
      "source": [
        "**Your answer:** ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZdPDQtdL8To"
      },
      "source": [
        "**Subtask 2 (0.5 points).** You can probably observe that the OpenSubtitles model has a much lower BLEU score on the OpenSubtitles test set than the DGT model does on the DGT test set. The models were trained in exactly the same way: the preprocessing pipeline is the same, the architecture is the same, the number of training sentence pairs is the same, the number of epochs is the same, only the corpora the sentences come from are different. How would you explain this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW-lQVVQL8Tp"
      },
      "source": [
        "**Your answer:** ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBifdHtwhpjQ"
      },
      "source": [
        "**Subtask 3 (0.5 points).** Look at 3-5 examples of sentences from the DGT test set translated with the OpenSubtitles model and vice versa. Compare them with the references. How bad are the translations? Do you see a pattern in what goes wrong?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGYul2s_nANe"
      },
      "source": [
        "**Your answer:** ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jakzuACdL8Tp"
      },
      "source": [
        "### Task 2: Beam search (2 points)\n",
        "\n",
        "During inference (translation), the model generates a probability distribution over the vocabulary at each step. For each word in the sentence that distribution depends on the previously generated words. To choose the most promising next step, beam search is used.\n",
        "\n",
        "**Subtask 1 (0.5 points).** Explain (in  4-5 sentences) what is beam search and why we need it for text generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6en62bwL8Tp"
      },
      "source": [
        "**Your answer:** ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B6szYh8L8Tq"
      },
      "source": [
        "Beam size is a parameter that does not depend on the model and can be set during translation. Typically, the default beam size is 5. \n",
        "\n",
        "**Subtask 2 (1.5 points).** Experiment with the external test set that you used in Homework 3. Translate the test set with your baseline model from Homework 2 using different beam sizes (for example, 1, 3, 5 and 10). Don't forget to postrocess your translations. Plot BLEU and translation time with different beam sizes (Fairseq reports translation time, which you can divide by the number of sentences in your test set). Describe what you see.\n",
        "\n",
        "If you do not have a well trained baseline model right now (if your model showed BLEU score less than 10 on the test set in Homework 3), use one of the models provided for Task 1.\n",
        "\n",
        "**Hint.** Do all translations from one SLURM script, then all translations will be done on the same node. Use the same batch size for all translations. This will ensure that the only thing that brings in the difference is the beam size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NVNpWV-L8Tq"
      },
      "source": [
        "**Your answer:** ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpZHYbFfL8Tr"
      },
      "source": [
        "### Task 3: Improving the baseline (2 points)\n",
        "\n",
        "In Homework 3 you summarized what kinds of mistakes your baseline model tends to make. In this task, you should come up with some ideas how to fix those mistakes.\n",
        "\n",
        "**Note.** If your model showed BLEU less than 10 on the test set from the previous homework, it means that something was wrong with the training. You should find what went wrong, retrain your model properly and redo the analysis. Otherwise it makes little sense for you to do this task. \n",
        "\n",
        "**Subtask 1 (2 points).** Propose some modifications to your model that you think could fix the typical mistakes your baseline model makes. Those modifications can be changes to the architecture, changes to the preprocessing pipeline, or other tricks you can come up with. Explain why you think your proposed modifications could fix the particular mistakes you mentioned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnuzhXekL8Tr"
      },
      "source": [
        "**Your answer:** ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79I8h4lWgPCP"
      },
      "source": [
        "### Task 4: Zindi MT competition (4 points)\n",
        "\n",
        "There is an ongoing MT competition on Zindi: [link](https://zindi.africa/competitions/ai4d-takwimu-lab-machine-translation-challenge/leaderboard). The aim of this competition is to automatically translate from French into two languages of the Niger-Congo family, Ewe and Fongbe. (You may remember this competition from the bonus task of Homework 1, which was to clean the competition data.)\n",
        "\n",
        "Your **task** is to sign up for this competition (if you haven't already), and make at least one submission. The main idea of this task is that you complete a full cycle of creating a machine translation system, from raw data to generating translations.\n",
        "\n",
        "The data provided by the organizers is quite small, so you can easily train your models in Colab, and training will not take a lot of time. Note that, according to the [competition rules](https://zindi.africa/competitions/ai4d-takwimu-lab-machine-translation-challenge), you may also use the [JW300 corpus](https://opus.nlpl.eu/JW300.php) if you wish. You can use publicly available pretrained models as well.\n",
        "\n",
        "You can choose any strategy you like (e.g. you can either train separate models for French-Ewe and French-Fongbe or one multilingual model for both language pairs, you can experiment with your models' architecture, etc.)\n",
        "\n",
        "Your leaderboard score doesn't have to be very high, but if you beat 0.281 you get **1 bonus point**. There is a starter notebook on Zindi; you will not get points if the only thing you do is run this notebook.\n",
        "\n",
        "**Subtask 1 (1 point).** Report the BLEU scores you get on your development set (the one you separated from the training data and use internally), and your Zindi username and public leaderboard score.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvCOp0RlN7ac"
      },
      "source": [
        "**Your answer:** ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EInbJFnS4AS"
      },
      "source": [
        "**Subtask 2 (3 points).** Describe what you did in detail: data, preprocessing, model hyperparameters, what problems you encountered, and anything else you would like to mention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrlrPLPwS5P8"
      },
      "source": [
        "**Your answer:** ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJso_NHyL8Ts"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}