{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:mtcourse]",
      "language": "python",
      "name": "conda-env-mtcourse-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "mt2021_hw2_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xclcis_1ELA"
      },
      "source": [
        "# Homework 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbQuvrAB1ELN"
      },
      "source": [
        "**How to submit.** Sumbissions are through the [courses page](https://courses.cs.ut.ee/2021/mt/spring/Main/Homeworks). For this homework, submit:\r\n",
        "\r\n",
        "1. this notebook with the description of your preprocessing and the output of translation;\r\n",
        "2. all the scripts you used for preprocessing, training and translation (please give your scripts descriptive names, e.g. `01_cleaning.sh`, `02_sentencepiece.sh`, and not `myscript.sh`);\r\n",
        "3. the configuration file, if you're using one (see the end of lab 3 materials for info on using Fairseq with configs);\r\n",
        "4. the binarization, training, and translation logs.\r\n",
        "\r\n",
        "**START IN ADVANCE.** Training the model will take several hours. There might be queues for the GPUs on Rocket as well, and waiting for your job to start might take much longer than the training itself. Leave yourself some room for error. You might want to use email notifications (`#SBATCH --mail-type=ALL` and `#SBATCH --mail-user=your@email` in your SLURM script). Then you will have more chance to notice if your job fails.\r\n",
        "\r\n",
        "**If you have your own GPU,** feel free to use it to train your model and not use HPC. You will need to adjust the maximum number of tokens in a batch to fit into your GPU's memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Duym8aRn1ELO"
      },
      "source": [
        "## Training an Estonian $\\rightarrow$ English translation model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rYMTnj_1ELP"
      },
      "source": [
        "In this homework, we will finally train an actual translation model.\n",
        "\n",
        "If you feel lost, review the materials of labs 2 and 3, and look into Fairseq's documentation. Feel free to ask a question in Slack (channel #homeworks) if something remains unclear.\n",
        "\n",
        "Note that you will need your trained model to do the next homework as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXkgRLn81ELP"
      },
      "source": [
        "### Task 1: Preprocessing (3 points)\n",
        "\n",
        "We will use the data provided in `/gpfs/hpc/projects/nlpgroup/mt2021/data/baseline-data`.\n",
        "\n",
        "First, preprocess the data:\n",
        "\n",
        "1. Concatenate and randomly shuffle the files (do not forget that parallel lines have to remain parallel after you shuffle them).\n",
        "2. Clean the data.\n",
        "  1. Apply basic cleaning: remove empty lines, very long sentences, and pairs with length ratio over 9. You can use the script in `/gpfs/hpc/projects/nlpgroup/mt2021/scripts/basic_cleaning.py` to do this, or you can use your own code that you wrote during lab 2.\n",
        "  2. (Optional) You can also apply your own cleaning procedures you came up with in Homework 1.\n",
        "3. Separate a development set of **2,000** sentence pairs and a test set of **2,000** sentence pairs, all remaining sentence pairs go into the training set.\n",
        "4. (Optional) Train a truecasing model and apply it.\n",
        "5. Train a SentencePiece model and apply it. The vocabulary size should be **32,000** units, **shared** between source and target sides (you need to train just one SentencePiece model, not two). You can use the script in `/gpfs/hpc/projects/nlpgroup/mt2021/scripts/apply_sentencepiece.py`, or your own code from lab 2.\n",
        "\n",
        "Note that you should only use the **training set** to train your truecasing and SentencePiece models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzsac5LnGrEb"
      },
      "source": [
        "**Describe what you did.** E.g. if you used truecasing or not, if you used additional cleaning or not, and any other details you think might be important.\r\n",
        "\r\n",
        "**Your answer:** ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWSPt9Cl1ELT"
      },
      "source": [
        "### Task 2: Training (5 points)\n",
        "\n",
        "Install Fairseq into a clean Conda environment, as we did in lab 3. Train a translation model with Fairseq, using the following hyperparameters:\n",
        "\n",
        "* use **Transformer** architecture in the encoder and decoder,\n",
        "* encoder and decoder should consist of **6 layers** each,\n",
        "* **8** attention heads,\n",
        "* initial learning rate is **$5\\cdot10^{-4}$**,\n",
        "* word embeddings in both encoder and decoder should be of size **256**,\n",
        "* embedding dimension for FFN (feed-forward layer) is **1024**,\n",
        "* **adam** optimizer,\n",
        "* **inverse square root** learning rate schedule with **4,000** warmup updates,\n",
        "* vocabulary is **shared** between source and target sides,\n",
        "* maximum number of tokens in one batch is **12,000**,\n",
        "* train your model for **20 epochs**.\n",
        "\n",
        "For all other hyperparameters, use the default values (don't change them in the config file if you are using `fairseq-hydra-train`, and don't specify them if using `fairseq-train`). Use 1 GPU on Rocket.\n",
        "\n",
        "If you do everything correctly, your model should have about 38-39 million parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tikctdgOuL4"
      },
      "source": [
        "If you want to mention something about how you trained the model, do it below.\r\n",
        "\r\n",
        "**Your answer:** ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5oBXCH01ELW"
      },
      "source": [
        "### Task 3: Applying the model (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsZL6B85MRA0"
      },
      "source": [
        "Translate the following sentence with the last checkpoint of your model:\r\n",
        "\r\n",
        "*Õppija põhiline eesmärk on teha üldistusi eelneva kogemustehulga põhjal.*\r\n",
        "\r\n",
        "Report the translation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iO0Chu1EM9qG"
      },
      "source": [
        "**Your answer:** ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lgAhB6a-Tvs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}